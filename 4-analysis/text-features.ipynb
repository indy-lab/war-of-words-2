{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import fasttext\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " # This analysis was done only for EP8 and the new_edit task\n",
    "    \n",
    "legislature = '8'\n",
    "task = 'new_edit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the embedding models trained on full data\n",
    "model_dir = '../data/text-embeddings'\n",
    "\n",
    "# Directory containing the canonical data\n",
    "data_dir = '../data/canonical'\n",
    "\n",
    "# Directory containing the dossier to title mapping\n",
    "titles_dir = '../data/helpers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# Loading saved fastText model for the edit and title to get the word and bigram embeddings\n",
    "model_edit = fasttext.load_model(model_dir + '/ep' + legislature + '-' + task + '-full' + '-edit.bin')\n",
    "model_title = fasttext.load_model(model_dir + '/ep' + legislature + '-' + task + '-full' + '-title.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the learned parameters corresponding to the edit and title text embedding\n",
    "weights_edit = np.loadtxt('edit-parameters.txt')\n",
    "weights_title = np.loadtxt('title-parameters.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "\n",
    "data = []\n",
    "with open(data_dir + '/war-of-words-2-ep' + legislature + '.txt','r') as json_file:\n",
    "    for line in json_file:\n",
    "        data.append(json.loads(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading mapping from dossier references to titles\n",
    "\n",
    "ref2title = {}\n",
    "with open(titles_dir+ '/dossier-titles.json','r') as json_file:\n",
    "    ref2title = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding dossier title to the edits and keeping track of missing dossiers\n",
    "\n",
    "missing_refs = set()\n",
    "\n",
    "for conflict in data:\n",
    "    for edit in conflict:\n",
    "        dossier_ref = edit['dossier_ref']\n",
    "        if dossier_ref in ref2title:\n",
    "            edit['dossier_title'] = ref2title[dossier_ref]\n",
    "        else:\n",
    "            edit['dossier_title'] = \"\"\n",
    "            missing_refs = missing_refs.union({dossier_ref})\n",
    "            \n",
    "if len(missing_refs) > 0:\n",
    "    print('Warning !', len(missing_refs),'references do not have an associated title!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _filter_dossiers(dataset, thr):\n",
    "    # Count occurence of each dossiers.\n",
    "    dossiers = list()\n",
    "    for data in dataset:\n",
    "        for datum in data:\n",
    "            dossiers.append(datum['dossier_ref'])\n",
    "    counter = Counter(dossiers)\n",
    "    # Define list of dossiers to keep.\n",
    "    keep = set([d for d, c in counter.items() if c > thr])\n",
    "    k, d = len(keep), len(set(dossiers))\n",
    "    print(f'Removed {d-k} ({(d-k)/d*100:.2f}%) dossiers.')\n",
    "    return keep\n",
    "def _filter_meps(dataset, thr):\n",
    "    # Count occurence of each dossiers.\n",
    "    meps = list()\n",
    "    for data in dataset:\n",
    "        for datum in data:\n",
    "            for at in datum['authors']:\n",
    "                meps.append(at['id'])\n",
    "    counter = Counter(meps)\n",
    "    # Define list of dossiers to keep.\n",
    "    keep = set([d for d, c in counter.items() if c > thr])\n",
    "    k, m = len(keep), len(set(meps))\n",
    "    print(f'Removed {m-k} ({(m-k)/m*100:.2f}%) MEPs.')\n",
    "    return keep\n",
    "def filter_dataset(dataset, thr=10):\n",
    "    \"\"\"Remove dossiers with less than `thr` edits.\"\"\"\n",
    "    keep_doss = _filter_dossiers(dataset, thr)\n",
    "    keep_mep = _filter_meps(dataset, thr)\n",
    "    filtered_dataset = list()\n",
    "    for data in dataset:\n",
    "        kd, km = True, True\n",
    "        for datum in data:\n",
    "            if datum['dossier_ref'] not in keep_doss:\n",
    "                kd = False\n",
    "            if not all(at['id'] in keep_mep for at in datum['authors']):\n",
    "                km = False\n",
    "        if kd and km:\n",
    "            filtered_dataset.append(data)\n",
    "    d, f = len(dataset), len(filtered_dataset)\n",
    "    print(f'Removed {d-f} ({(d-f)/d*100:.2f}%) conflicts.')\n",
    "    print('Number of data points:', len(filtered_dataset))\n",
    "    return filtered_dataset\n",
    "\n",
    "def unroll(dataset):\n",
    "    unrolled = list()\n",
    "    for conflict in dataset:\n",
    "        for edit in conflict:\n",
    "            unrolled.append(edit)\n",
    "    return unrolled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 51 (6.38%) dossiers.\n",
      "Removed 14 (1.77%) MEPs.\n",
      "Removed 271 (0.19%) conflicts.\n",
      "Number of data points: 140763\n"
     ]
    }
   ],
   "source": [
    "data_filtered = filter_dataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered_unrolled = unroll(data_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bigrams(unrolled_data):\n",
    "    bigrams = set()\n",
    "    for j,datum in enumerate(unrolled_data):\n",
    "\n",
    "        i1 = datum['edit_indices']['i1']\n",
    "        i2 = datum['edit_indices']['i2']\n",
    "        j1 = datum['edit_indices']['j1']\n",
    "        j2 = datum['edit_indices']['j2']\n",
    "\n",
    "        text_del = datum['text_original'][i1:i2]\n",
    "        text_ins = datum['text_amended'][j1:j2]\n",
    "        text_context_l = datum['text_original'][:i1] \n",
    "        text_context_r = datum['text_original'][i2:]\n",
    "\n",
    "\n",
    "        if j%10000==0:\n",
    "            print(j)\n",
    "        if len(text_del) > 1:\n",
    "            org_text_with_tag = ['<del>'+w for w in text_del]\n",
    "            for i in range(len(org_text_with_tag)-1):\n",
    "                bigrams.add(' '.join([org_text_with_tag[i],org_text_with_tag[i+1]]))\n",
    "        if len(text_ins) > 1:\n",
    "            org_text_with_tag = ['<ins>'+w for w in text_ins]\n",
    "            for i in range(len(org_text_with_tag)-1):\n",
    "                bigrams.add(' '.join([org_text_with_tag[i],org_text_with_tag[i+1]]))\n",
    "        if len(text_context_l) > 1:\n",
    "            org_text_with_tag = ['<con>'+w for w in text_context_l]\n",
    "            for i in range(len(org_text_with_tag)-1):\n",
    "                bigrams.add(' '.join([org_text_with_tag[i],org_text_with_tag[i+1]]))\n",
    "        if len(text_context_r) > 1:\n",
    "            org_text_with_tag = ['<con>'+w for w in text_context_r]\n",
    "            for i in range(len(org_text_with_tag)-1):\n",
    "                bigrams.add(' '.join([org_text_with_tag[i],org_text_with_tag[i+1]]))       \n",
    "\n",
    "        title_word_list = [re.sub('\\d','D',w.lower()) for w in word_tokenize(datum['dossier_title'])]\n",
    "        if len(title_word_list) > 1:\n",
    "            for i in range(len(title_word_list)-1):\n",
    "                bigrams.add(' '.join([title_word_list[i],title_word_list[i+1]]))\n",
    "    bigrams = list(bigrams)\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_denominator(bigram,model_obj):\n",
    "    \n",
    "    # Get number of words in the bigram that is in the vocabulary\n",
    "    n_words = 0\n",
    "    words = bigram.split()\n",
    "    for w in words:\n",
    "        if model_obj.get_word_id(w) > -1:\n",
    "            n_words += 1\n",
    "    \n",
    "    n_bigrams = len(words)\n",
    "    \n",
    "    # Consider 'w1 w2 EOS' - the averaging is over the words among w1 and w2 that are in the vocabulary (n_words), \n",
    "    # EOS token (1), and the number of bigrams - (w1, w2) and (w2, EOS) (which is len(words)=2) \n",
    "    denominator = n_words + 1 + n_bigrams\n",
    "    \n",
    "    return denominator\n",
    "    \n",
    "\n",
    "def get_bigram_vector(bigram,model_obj):\n",
    "    \n",
    "    # Getting the bigram vector in a roundabout way from the fasttext model object\n",
    "    \n",
    "    words = bigram.split()\n",
    "    \n",
    "    sent_vec = model_obj.get_sentence_vector(bigram)\n",
    "    \n",
    "    denom = get_denominator(bigram,model_obj)\n",
    "    \n",
    "    # Reversing the averaging operation to get the sum\n",
    "    sum_vec = sent_vec*denom\n",
    "    \n",
    "    sub_sent_vec = model_obj.get_sentence_vector(words[1])\n",
    "    \n",
    "    if model_obj.get_word_id(words[1]) > -1:\n",
    "        # If word is part of vocabulary the average is over the word itself, EOS token and word+EOS\n",
    "        sub_sent_vec = sub_sent_vec*3\n",
    "    else:\n",
    "        sub_sent_vec = sub_sent_vec*2\n",
    "    \n",
    "    if model_obj.get_word_id(words[0]) > -1:\n",
    "        return sum_vec - sub_sent_vec - model_obj.get_word_vector(words[0])\n",
    "    else:\n",
    "        return sum_vec - sub_sent_vec\n",
    "    \n",
    "def get_topk_bigram(bigrams,model_obj,weights,ftype='added',outcome='accept',k=50):\n",
    "    if ftype=='removed':\n",
    "        bigram_indices = [i for (i,bw) in enumerate(bigrams) if bw[:5]=='<del>']\n",
    "    elif ftype=='added':\n",
    "        bigram_indices = [i for (i,bw) in enumerate(bigrams) if bw[:5]=='<ins>']\n",
    "    elif ftype=='context':\n",
    "        bigram_indices = [i for (i,bw) in enumerate(bigrams) if bw[:5]=='<con>']\n",
    "    elif ftype=='title':\n",
    "        bigram_indices = [i for (i,bw) in enumerate(bigrams) if (bw[:5]!='<del>' and bw[:5]!='<ins>' and bw[:5]!='<con>')]\n",
    "    else:\n",
    "        print('Invalid feature type')\n",
    "        return []\n",
    "        \n",
    "    #print('Collected bigram indices')\n",
    "\n",
    "    bigrams = np.array(bigrams)\n",
    "    bigram_indices = np.array(bigram_indices)\n",
    "    \n",
    "    bigram_vectors = np.array([get_bigram_vector(bigrams[i],model_obj) for i in bigram_indices])\n",
    "\n",
    "    #print('Collected bigram vectors')\n",
    "\n",
    "    dotprods = bigram_vectors.dot(weights)\n",
    "    \n",
    "    #print('Computed dot products')\n",
    "    \n",
    "    argsorted_dotprods = np.argsort(dotprods)\n",
    "    \n",
    "    if ftype=='title':\n",
    "        argsorted_dotprods = argsorted_dotprods\n",
    "    else:\n",
    "        argsorted_dotprods = argsorted_dotprods[::-1]\n",
    "    \n",
    "    if outcome=='accept':\n",
    "        argsorted_dotprods = argsorted_dotprods\n",
    "    elif outcome=='reject':\n",
    "        argsorted_dotprods = argsorted_dotprods[::-1]\n",
    "    else:\n",
    "        print('Invalid outcome')\n",
    "        return []\n",
    "    \n",
    "    if ftype=='title':\n",
    "        bigram_list = bigrams[bigram_indices[argsorted_dotprods]][:k]\n",
    "    else:\n",
    "        bigram_list = remove_tags(bigrams[bigram_indices[argsorted_dotprods]][:k])\n",
    "    \n",
    "    return bigram_list   \n",
    "\n",
    "\n",
    "def get_topk_word(model_obj,weights,ftype='added',outcome='accept',k=50):\n",
    "    # Get vocabulary\n",
    "    vocab = model_obj.get_words()\n",
    "    \n",
    "    # Get input matrix (the word embeddings)\n",
    "    im = model_obj.get_input_matrix()\n",
    "    \n",
    "    if ftype=='removed':\n",
    "        word_indices = [i for (i,w) in enumerate(vocab) if w[:5]=='<del>']\n",
    "    elif ftype=='added':\n",
    "        word_indices = [i for (i,w) in enumerate(vocab) if w[:5]=='<ins>']\n",
    "    elif ftype=='context':\n",
    "        word_indices = [i for (i,w) in enumerate(vocab) if w[:5]=='<con>']\n",
    "    elif ftype=='title':\n",
    "        word_indices = [i for (i,w) in enumerate(vocab)]\n",
    "    else:\n",
    "        print('Invalid ftype')\n",
    "        return []\n",
    "\n",
    "\n",
    "    word_indices = np.array(word_indices)\n",
    "\n",
    "    word_vectors = im[word_indices,:]\n",
    "\n",
    "    dotprods = word_vectors.dot(weights)\n",
    "    \n",
    "    argsorted_dotprods = np.argsort(dotprods)\n",
    "    \n",
    "    # The ordering is reversed for the title as it is in the denominator of the edit acceptance probability (see eq.(7) in the paper)\n",
    "    if ftype=='title':\n",
    "        argsorted_dotprods = argsorted_dotprods\n",
    "    else:\n",
    "        argsorted_dotprods = argsorted_dotprods[::-1]\n",
    "    \n",
    "    if outcome=='accept':\n",
    "        argsorted_dotprods = argsorted_dotprods\n",
    "    elif outcome=='reject':\n",
    "        argsorted_dotprods = argsorted_dotprods[::-1]\n",
    "    else:\n",
    "        print('Invalid outcome')\n",
    "        return []\n",
    "    vocab = np.array(vocab)\n",
    "    \n",
    "    if ftype=='title':\n",
    "        word_list = vocab[word_indices[argsorted_dotprods]][:k]\n",
    "    else:\n",
    "        word_list = remove_tags(vocab[word_indices[argsorted_dotprods]][:k])\n",
    "    \n",
    "    return word_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(l):\n",
    "    # Given a list of words or bigrams, returns the list with the tags removed from each element\n",
    "    l_new = []\n",
    "    for wb in l:\n",
    "        wl = wb.split()\n",
    "        wb_new = ''\n",
    "        for w in wl:\n",
    "            wb_new += w[5:] \n",
    "            wb_new += ' '\n",
    "        wb_new = wb_new[:-1]\n",
    "        l_new.append(wb_new)\n",
    "    return l_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n"
     ]
    }
   ],
   "source": [
    "bigrams = extract_bigrams(data_filtered_unrolled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words and Bigrams Predictive of Acceptance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'berec | fishing | should | office | registered | 2018 | inserted | equipment | actions | transparency | important | advisory | fisheries | x | bargaining | best | processes | fuel | financial | regulators | ” | communication | pension | agricultural | supervisory | positive | gender | creative | reduce | plan | impact | withdrawal | external | eets | second | investigation | procurement | ppe | improves | blue | skills | common | hubs | toll | therefore | indicators | contributions | 20 | circular | lisa'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Words Added\n",
    "\n",
    "' | '.join(get_topk_word(model_edit,weights_edit,'added','accept',50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'safety | berec | consumers | eurojust | area | breeding | council | surveillance | powers | 2 | human | authorised | animals | bodies | hosting | ; | conditions | articles | annexes | if | added | medium | provision | origin | fisheries | representative |  | benefitting | manufacturer | conformity | fitting | derogation | 29 | plant | virtual | action | recommendation | sex | breed | chapter | amending | current | processing | specific | point | during | implementation | 2025 | covered | financing'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Words Removed\n",
    "\n",
    "' | '.join(get_topk_word(model_edit,weights_edit,'removed','accept',50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'” | appliance | prima | rco | harmonised | appliances | threats | breeding | controls | voice | alternative | eurojust | safety | egf | published | processed | outside | fitting | instructions | 63 | accounts | associated | institutions | destination | observations | cash | recipients | engines | creditors | berec | manufacturer | expenditure | customs | guidelines | appeal | alcohol | cableway | name | secretariat | multi | iccat | instrument | number | positive | audit | document | uniform | operating | notified | stock'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Context Words\n",
    "\n",
    "' | '.join(get_topk_word(model_edit,weights_edit,'context','accept',50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"customs | community | mediterranean | DDDD-DDDD | supervision | service | control | installations | 'customs | parliament | cableway | equipment | pollutants | anti-fraud | annex | competition | multiannual | council | statistics | temporary | recovery | documents | field | area | fuels | gaseous | DDDD/DDD | appliances | policy | drinks | plan | ukraine | genealogical | animals | germinal | financial | burning | fisheries | spirit | context | laws | investigations | it | management | other | ensure | medicines | DD/DDDD | agency | insolvency\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Title Words\n",
    "\n",
    "' | '.join(get_topk_word(model_title,weights_title,'title','accept',50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'their sector | opposition , | the berec | of meeting | , humification | this regulation | berec office | this expert | avoid social | way behind | a . | in easy | transmission of | were neither | eu´s greenhouse | violence is | applicable the | one of | within the | . 2 | economic operators | relative deviation | where applicable | risk premia | positive impact | further amended | accept , | is inserted | not properly | multinationals at | institution , | by sub | the third | regions , | or federal | family associations | intelligent mobility | - carrier | carrier economic | signal processing | acoustic signals | residency or | complaint was | , raising | , 51 | people and | assessment of | board of | care on | the commission'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bigrams Added\n",
    "\n",
    "' | '.join(get_topk_bigram(bigrams,model_edit,weights_edit,'added','accept',50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2 . | . 2 | . . | international efforts | . where | , member | the case | the member | hosting service | the following | 5 . | human rights | ; the | market surveillance | data protection | in such | 4 . | to be | requirements of | the hosting | which are | : the | subject to | whether the | to that | and related | service provider | of that | provided for | evaluations ; | plan ; | covered by | . 3 | eurojust shall | relevant for | conditions , | sharing and | the council | 6 . | notified as | the implementing | the development | directive . | take a | application for | the efsd | article 11 | the data | in hormonal | of consumers'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bigrams Removed\n",
    "\n",
    "' | '.join(get_topk_bigram(bigrams,model_edit,weights_edit,'removed','accept',50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\". ' | : ' | . those | . 3 | 2 , | notified body | . 2 | . ” | is in | authority to | under other | management board | requirements of | year . | their citizenship | annual work | renewable energy | public sector | ' the | supervisory authorities | promoter . | within a | be deferred | investment firms | economic operators | the egf | 3 . | of new | voice communications | shall : | and shall | authorities should | / 22 | in other | 4 . | quantified , | , storage | this paragraph | plan . | financing types | regulation . | consumers ' | commission in | the market | of participants | resident or | monitor the | ' interests | article 38 | states to\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Context Bigrams\n",
    "\n",
    "' | '.join(get_topk_bigram(bigrams,model_edit,weights_edit,'context','accept',50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"council on | to regulation | cableway installations | supervision of | , ( | ' programme | 'customs ' | of customs | recovery plan | european parliament | multiannual recovery | and of | general budget | the mediterranean | annex a | the 'customs | the field | and establishing | parliament and | on insolvency | replacing annex | insolvency proceedings | a to | and administrative | regulation of | rules applicable | budget of | customs control | control equipment | burning gaseous | field of | the reform | gaseous fuels | DDDD/DDD on | financial rules | for cooperation | the use | zootechnical and | plan for | and supervision | for trade | the council | appliances burning | procedures for | in and | no DD/DDDD | DDDD/DDDD , | imports into | union of | to ensure\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Title Bigrams\n",
    "\n",
    "' | '.join(get_topk_bigram(bigrams,model_title,weights_title,'title','accept',50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words and Bigrams Predictive of Rejection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'these | cabotage | deleted | ; | eu | except | societal | &#160 | territorial | illegal | – | payment | mercury | must | asylum | hydrogen | e | benchmark | commercial | include | according | service | benefits | determined | operational | solidarity | negative | binding | circumstances | professionals | firearms | consent | case | participants | interest | ) | ten | days | settlement | after | basic | children | s | if | defined | additionality | agreements | amended | deputy | roma'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Words Added\n",
    "\n",
    "' | '.join(get_topk_word(model_edit,weights_edit,'added','reject',50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'energy | should | migration | additional | public | corps | workers | competitiveness | irregular | different | product | % | joint | systems | forest | worker | remuneration | international | research | eib | efsi | before | growth | economic | passenger | electronic | cultural | solidarity | therefore | matter | months | online | impact | works | concerted | waste | through | can | reporting | value | building | eurodac | europe | allowances | identity | more | account | return | objectives | facial'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Words Removed\n",
    "\n",
    "' | '.join(get_topk_word(model_edit,weights_edit,'removed','reject',50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_________________ | allocation | resettlement | posting | benchmark | allowances | rightholders | allocations | firearms | reserve | foreign | hosting | free | driver | pnr | ) | educational | core | verification | labels | works | ancillary | forest | collective | advanced | broadcast | terrorist | investments | fine | excellence | 25 | parental | condition | preservation | mercury | 2030 | million | renovation | remote | employees | fingerprints | settings | redress | exception | travel | target | solidarity | sectors | 00 | aims'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Context Words\n",
    "\n",
    "' | '.join(get_topk_word(model_edit,weights_edit,'context','reject',50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and | directive | market | DDDD | framework | services | DDDD/DDD/ec | </s> | agricultural | requirements | energy | protection | a | for | as | gas | greenhouse | contracts | decision | online | name | operation | digital | regulation | strategic | view | development | of | at | structural | emission | instruments | establishment | trading | the | record | from | in | regards | supplementary | specific | plans | private | posting | screening | DDD/DDDD | pnr | certificate | under | investments'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Title Words\n",
    "\n",
    "' | '.join(get_topk_word(model_title,weights_title,'title','reject',50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"“10a . | communication , | welfare regulations | normalisation process | become apparent | that activity | is deleted | certificates were | general production | &#160 ; | may propose | and in | separation of | ' s | with the | and logistics | hatred . | . in | or morality | as jointly | different generators | fuels for | engine replacement | procedures overcoming | according to | as authors | is amended | directly awarding | annex , | european union | lifting a | parties to | - contributions | ii may | , except | made explicitly | place of | value cases | in any | employer shall | judges each | or new | operations of | are non | between solid | , point | , for | 32a is | new genetic | the annex\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bigrams Added\n",
    "\n",
    "' | '.join(get_topk_bigram(bigrams,model_edit,weights_edit,'added','reject',50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"no reason | and now | as the | terrorism - | contribute to | , possessed | under the | digital content | other subject | guidance , | electronic monitoring | legal body | . this | the digital | , shall | which the | the cir | remote electronic | 0 . | than ten | . member | agricultural guarantee | solidarity corps | in case | the passenger | authorised periods | intention or | and other | the supplier | information on | of directive | 1 . | policy objectives | . in | least likely | the product | discussion . | and of | investigations or | - sharing | ' association | state to | - matter | - user | carbon impact | the forest | 2 of | same shall | purpose of | ] and\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bigrams Removed\n",
    "\n",
    "' | '.join(get_topk_bigram(bigrams,model_edit,weights_edit,'removed','reject',50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"contents of | a sub | therefore , | hosting service | . _________________ | of directive | the funds | pnr data | article 4 | investment board | parental leave | regional operational | report within | state which | commission may | paragraph 3 | scientific evaluation | works or | main third | produced from | the driver | procedure , | 000 for | their common | they shall | have given | states may | or other | ' shall | free allocation | deemed to | 27 . | states introduce | commission should | which establishes | - and | their rights | for free | data for | programme’s research | finance may | - use | down rules | consumers , | countries in | . member | be deemed | article 2 | / 123 | 123 /\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Context Bigrams\n",
    "\n",
    "' | '.join(get_topk_bigram(bigrams,model_edit,weights_edit,'context','reject',50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and regulation | ) and | directive DDDD/DDD/ec | greenhouse gas | rules for | european union | corps programme | services in | , regulation | the eu | DDDD/DDD and | eu pnr | by member | passenger name | record data | data ( | structural reform | the structural | of passenger | name record | strategic plans | to georgia | european agricultural | the framework | pnr ) | DDDD/DDDD with | DDD/DDDD as | and weekly | daily and | DDDD/DDD/ec and | efficiency labelling | screening of | developments in | concerning the | , and | directive DDDD/DD/eu | direct investments | internal market | , laying | rules on | council amending | of a | gas emission | as regards | of energy | to adapting | for medicinal | supplementary protection | protection certificate | of water'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Title Bigrams\n",
    "\n",
    "' | '.join(get_topk_bigram(bigrams,model_title,weights_title,'title','reject',50))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
